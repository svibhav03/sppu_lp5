{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvCrWXBPjs8O",
        "outputId": "0c50dd1a-9eaa-44ea-d646-3568d8edfcda"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon May  5 18:50:56 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jbR2JwHjxTw",
        "outputId": "a6959933-3a7e-4dcc-f34f-414254d2c9c0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_CsIfBLj2_y",
        "outputId": "4ea74179-5551-4909-a5ce-cf42d8a716a2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrBueqO_j8xS",
        "outputId": "22961f62-e32f-4f65-eeda-d8103c13ff37"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmpsg8e8n9o\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZhYOhGYXjjHW"
      },
      "outputs": [],
      "source": [
        "code = r\"\"\"\n",
        "#include <iostream>\n",
        "#include <climits>\n",
        "#include <chrono>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "__global__ void reduceMin(int* input, int* output, int size) {\n",
        "    __shared__ int sdata[BLOCK_SIZE];\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    sdata[tid] = (i < size) ? input[i] : INT_MAX;\n",
        "    __syncthreads();\n",
        "\n",
        "    for (unsigned int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n",
        "        if (tid < stride) {\n",
        "            sdata[tid] = min(sdata[tid], sdata[tid + stride]);\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0) output[blockIdx.x] = sdata[0];\n",
        "}\n",
        "\n",
        "__global__ void reduceMax(int* input, int* output, int size) {\n",
        "    __shared__ int sdata[BLOCK_SIZE];\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    sdata[tid] = (i < size) ? input[i] : INT_MIN;\n",
        "    __syncthreads();\n",
        "\n",
        "    for (unsigned int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n",
        "        if (tid < stride) {\n",
        "            sdata[tid] = max(sdata[tid], sdata[tid + stride]);\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0) output[blockIdx.x] = sdata[0];\n",
        "}\n",
        "\n",
        "__global__ void reduceSum(int* input, int* output, int size) {\n",
        "    __shared__ int sdata[BLOCK_SIZE];\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    sdata[tid] = (i < size) ? input[i] : 0;\n",
        "    __syncthreads();\n",
        "\n",
        "    for (unsigned int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n",
        "        if (tid < stride) {\n",
        "            sdata[tid] += sdata[tid + stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0) output[blockIdx.x] = sdata[0];\n",
        "}\n",
        "\n",
        "inline cudaError_t checkCudaError(cudaError_t err, const char* msg) {\n",
        "    if (err != cudaSuccess) {\n",
        "        std::cerr << \"CUDA Error: \" << msg << \" - \" << cudaGetErrorString(err) << std::endl;\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "    return err;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int size = 1 << 10;\n",
        "    int* h_input = (int*)malloc(size * sizeof(int));\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        h_input[i] = rand() % 100 + 1;\n",
        "    }\n",
        "\n",
        "    int cpu_min = INT_MAX, cpu_max = INT_MIN, cpu_sum = 0;\n",
        "    auto start_cpu = std::chrono::high_resolution_clock::now();\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        cpu_min = std::min(cpu_min, h_input[i]);\n",
        "        cpu_max = std::max(cpu_max, h_input[i]);\n",
        "        cpu_sum += h_input[i];\n",
        "    }\n",
        "    auto end_cpu = std::chrono::high_resolution_clock::now();\n",
        "    auto cpu_time = std::chrono::duration_cast<std::chrono::milliseconds>(end_cpu - start_cpu).count();\n",
        "    float cpu_avg = (float)cpu_sum / size;\n",
        "\n",
        "    int *d_input, *d_output_min, *d_output_max, *d_output_sum;\n",
        "    int gridSize = (size + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
        "\n",
        "    checkCudaError(cudaMalloc((void**)&d_input, size * sizeof(int)), \"Malloc d_input\");\n",
        "    checkCudaError(cudaMalloc((void**)&d_output_min, gridSize * sizeof(int)), \"Malloc d_output_min\");\n",
        "    checkCudaError(cudaMalloc((void**)&d_output_max, gridSize * sizeof(int)), \"Malloc d_output_max\");\n",
        "    checkCudaError(cudaMalloc((void**)&d_output_sum, gridSize * sizeof(int)), \"Malloc d_output_sum\");\n",
        "\n",
        "    checkCudaError(cudaMemcpy(d_input, h_input, size * sizeof(int), cudaMemcpyHostToDevice), \"Memcpy input\");\n",
        "\n",
        "    cudaEvent_t start_gpu, stop_gpu;\n",
        "    checkCudaError(cudaEventCreate(&start_gpu), \"Create event start\");\n",
        "    checkCudaError(cudaEventCreate(&stop_gpu), \"Create event stop\");\n",
        "\n",
        "    cudaEventRecord(start_gpu);\n",
        "\n",
        "    reduceMin<<<gridSize, BLOCK_SIZE>>>(d_input, d_output_min, size);\n",
        "    reduceMax<<<gridSize, BLOCK_SIZE>>>(d_input, d_output_max, size);\n",
        "    reduceSum<<<gridSize, BLOCK_SIZE>>>(d_input, d_output_sum, size);\n",
        "\n",
        "    checkCudaError(cudaGetLastError(), \"Kernel launch failed\");\n",
        "    cudaEventRecord(stop_gpu);\n",
        "    cudaEventSynchronize(stop_gpu);\n",
        "\n",
        "    float gpu_time = 0.0f;\n",
        "    cudaEventElapsedTime(&gpu_time, start_gpu, stop_gpu);\n",
        "\n",
        "    int* h_output_min = (int*)malloc(gridSize * sizeof(int));\n",
        "    int* h_output_max = (int*)malloc(gridSize * sizeof(int));\n",
        "    int* h_output_sum = (int*)malloc(gridSize * sizeof(int));\n",
        "\n",
        "    checkCudaError(cudaMemcpy(h_output_min, d_output_min, gridSize * sizeof(int), cudaMemcpyDeviceToHost), \"Memcpy min output\");\n",
        "    checkCudaError(cudaMemcpy(h_output_max, d_output_max, gridSize * sizeof(int), cudaMemcpyDeviceToHost), \"Memcpy max output\");\n",
        "    checkCudaError(cudaMemcpy(h_output_sum, d_output_sum, gridSize * sizeof(int), cudaMemcpyDeviceToHost), \"Memcpy sum output\");\n",
        "\n",
        "    int gpu_min = INT_MAX, gpu_max = INT_MIN, gpu_sum = 0;\n",
        "    for (int i = 0; i < gridSize; i++) {\n",
        "        gpu_min = std::min(gpu_min, h_output_min[i]);\n",
        "        gpu_max = std::max(gpu_max, h_output_max[i]);\n",
        "        gpu_sum += h_output_sum[i];\n",
        "    }\n",
        "    float gpu_avg = (float)gpu_sum / size;\n",
        "\n",
        "    std::cout << \"\\nCPU Results:\\n\";\n",
        "    std::cout << \"Min: \" << cpu_min << \" | Max: \" << cpu_max << \" | Sum: \" << cpu_sum\n",
        "              << \" | Avg: \" << cpu_avg << \" | Time: \" << cpu_time << \" ms\\n\";\n",
        "\n",
        "    std::cout << \"\\nGPU Results:\\n\";\n",
        "    std::cout << \"Min: \" << gpu_min << \" | Max: \" << gpu_max << \" | Sum: \" << gpu_sum\n",
        "              << \" | Avg: \" << gpu_avg << \" | Time: \" << gpu_time << \" ms\\n\";\n",
        "\n",
        "    free(h_input);\n",
        "    free(h_output_min);\n",
        "    free(h_output_max);\n",
        "    free(h_output_sum);\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output_min);\n",
        "    cudaFree(d_output_max);\n",
        "    cudaFree(d_output_sum);\n",
        "    cudaEventDestroy(start_gpu);\n",
        "    cudaEventDestroy(stop_gpu);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "with open(\"main.cu\", \"w\") as f:\n",
        "    f.write(code)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 main.cu -o main"
      ],
      "metadata": {
        "id": "niWSdUYfmK5U"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!./main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdRI7LzRy4hx",
        "outputId": "762383cc-c314-48b8-a969-d9d445150fa6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CPU Results:\n",
            "Min: 1 | Max: 100 | Sum: 52557 | Avg: 51.3252 | Time: 0 ms\n",
            "\n",
            "GPU Results:\n",
            "Min: 1 | Max: 100 | Sum: 52557 | Avg: 51.3252 | Time: 0.165888 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L2EckxTszcAV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}